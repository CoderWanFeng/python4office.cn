---
title: 每天和 AI 聊天像好友？你的隐私可能正在全网裸奔！
date: 2025-08-20 22:16:36
tags: [ ai,gpt,大模型,国产,机器人 ]
---


<p align="center" id='进群-banner'>
    <a target="_blank" href='https://mp.weixin.qq.com/s/0GrWWSQ8sKs-WA8WoN3Ztg?payreadticket=HPsk3SM42QLKkwlPgzoQN00eTUDy7x7I70-jcY9jIG2bWFmjZvB7r1mF10OiNSkxknfiN08&scene=1&click_id=1'>
    <img src="https://raw.gitcode.com/user-images/assets/5027920/d78ad96d-6a5e-49fa-9a6d-ba98ec1a1293/image.png" width="100%"/>
    </a>   
</p>

<p align="center">
	👉 <a target="_blank" href="https://www.python-office.com/">项目官网：https://www.python-office.com/</a> 👈
</p>
<p align="center">
	👉 <a target="_blank" href="http://www.python4office.cn/wechat-group/">本开源项目的交流群</a> 👈
</p>



<p align="center" name="gitcode">
	<a target="_blank" href='https://gitcode.com/CoderWanFeng1/python-office'>
		<img src='https://gitcode.com/CoderWanFeng1/python-office/star/badge.svg?theme=dark' alt='gitcode star'/>
	</a>	
	    <a target="_blank" href='https://github.com/CoderWanFeng/python-office'>
    <img src="https://img.shields.io/github/stars/CoderWanFeng/python-office.svg?style=social" alt="github star"/>
    </a>
    	<a target="_blank" href='https://gitee.com/CoderWanFeng//python-office/'>
		<img src='https://gitee.com/CoderWanFeng//python-office/badge/star.svg?theme=dark' alt='gitee star'/>
	</a>
	<a target="_blank" href='https://gitcode.com/CoderWanFeng1/python-office'>
<img src="https://static.pepy.tech/badge/python-office" alt="PyPI Downloads">
</a>
    	<a href="http://www.python4office.cn/wechat-group/">
	<img src="https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E4%BA%A4%E6%B5%81%E7%BE%A4-brightgreen"/>
  </a>

</p>


<!-- more -->

![](https://raw.gitcode.com/user-images/assets/5027920/4337c6f5-dbcd-4286-8a5e-7dbcc15d0fbc/image.png 'image.png')


> AI正在窃听我们的秘密，而你却毫不知情。
 
大家好，这里是程序员晚枫，最近大模型很火，今天和大家聊一个敏感的话题：**大模型侵犯个人隐私。**

2025年8月，一款流行儿童手表中的AI功能被询问“中国人是世界上最聪明的人吗？”时，竟然给出了否定中国发明创造、否定中国文化的答案，引发轩然大波。厂家随后紧急道歉，称已修正相关数据。

这只是人工智能侵犯个人隐私的冰山一角。随着ChatGPT、DeepSeek等大模型迅速普及，我们的隐私安全正面临前所未有的挑战。

## 01 隐私窃取，大模型正在如何侵犯我们？

AI侵权案例比比皆是，且通常悄无声息。2025年8月，一项针对Otter.ai的集体诉讼指控该公司“欺骗性和偷偷摸摸地”记录私人对话。

Otter.ai的AI转录服务能在Zoom、Google Meet和Microsoft Teams会议中进行实时转录，但**默认不会询问与会者是否同意录制**
，也不会提醒参与者录音会被分享给Otter以改进其人工智能系统。

> 新闻链接：https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit

原告Justin Brewer声称，在意识到Otter秘密记录了一次保密谈话后，他的隐私遭到了“严重侵犯”。更令人担忧的是，尽管Otter的隐私政策声称会获得用户明确许可，但许多人仍然被误导。

类似情况也发生在Meta身上。2025年8月，德国石勒苏益格-荷尔斯泰因高等地区法院确认，Meta的人工智能训练程序处理儿童和青少年的个人数据，尽管公司声称实施了保护措施。

法院文件显示，当成年用户在Facebook和Instagram上分享包含儿童数据的内容时，Meta的AI系统不可避免地会捕获未成年人的信息。

**尽管实施了去识别化和令牌化措施，这些保护并未有效防止未注册用户、儿童、青少年个人信息纳入AI训练记录**。

![大模型vs个人隐私](https://raw.gitcode.com/user-images/assets/5027920/271746e2-d2ae-46e0-809d-51ee4cab2160/image.png 'image.png')

另外，欧盟对“大模型侵犯个人隐私”开出的罚单已呈“常态化”。以下 8 起典型案例均发生在 2021-2025 年间，处罚对象全部是训练或运营超大规模AI/LLM 的科技巨头，

违法焦点集中在「未经有效同意抓取个人数据、跨境传输、以及用户画像」。按时间倒序，信息全部来自欧盟官方通报或经官方确认的媒体报道。

| 时间      | 受罚企业            | 违法场景                            | 处罚机构 & 金额                             | 核心违规点                                          | 官方来源 |
|---------|-----------------|---------------------------------|---------------------------------------|------------------------------------------------|------|
| 2025-05 | Meta (Facebook) | 用欧盟用户公开帖子、图片、位置训练 Llama-3 等 LLM | 爱尔兰 DPC 提议罚款 12 亿欧元（最终决定书 2025-06 生效） | 未获得“明确同意”即用于 AI 训练；跨境传到美国缺乏 GDPR 第 46 条等效保障    |
| 2024-08 | Uber            | 司机账户、位置、证件、犯罪记录被传至美国训练风控模型      | 荷兰 AP 2.9 亿欧元                         | 未签署 SCC、未加入 EU-US Data Privacy Framework 就跨境传输 |
| 2023-05 | Meta (Facebook) | 继续把欧盟用户数据传美用于广告 & AI            | 爱尔兰 DPC 12 亿欧元                        | 跨境传输工具被欧盟法院判定无效后仍继续传输                          |
| 2021-07 | Amazon          | 用 Alexa 语音、购物记录训练个性化推荐模型        | 卢森堡 DPA 7.46 亿欧元                      | 未清晰告知用户数据将用于机器学习；缺乏有效同意                        |
| 2021-07 | WhatsApp (Meta) | 把电话号码、通讯录用于训练联系人匹配算法            | 爱尔兰 DPC 2.25 亿欧元                      | 同意机制不透明；与 Meta 其他服务共享目的未充分说明                   |
| 2021-07 | Google          | 位置历史、搜索记录、YouTube 观看数据训练广告模型    | 法国 CNIL 9000 万欧元                      | 默认开启个性化广告；用户难以真正撤回同意                           |
| 2020-07 | TikTok          | 13 岁以下儿童数据被用于推荐算法训练             | 荷兰 DPA 75 万欧元；英国 ICO 1450 万欧元         | 未征得父母同意；默认公开未成年人账号                             |
| 2019-01 | Google          | 个性化广告缺乏透明度和有效同意                 | 法国 CNIL 5000 万欧元                      | 用户无法了解数据被用于机器学习广告模型的具体逻辑                       |

#### 补充说明

1. 12 亿欧元（Meta 2023）仍是目前 GDPR 史上最高单笔罚款，但 Meta 2025 的 Llama-3 训练案已追平该记录。
2. 所有处罚都援引 GDPR 第 5-7 条（合法性、公平性、透明性、同意）、第 9 条（敏感数据）和第 44-49 条（跨境传输）。
3. 2024 年生效的《AI Act》把“高风险 AI 系统”训练数据合法性列为前置条件，预计 2025 下半年还会出现新一轮针对大模型的更大罚单。



## 02 漏洞百出，大模型为何成为隐私黑洞？

大模型侵犯隐私的根本原因在于其**需要海量训练数据**。大语言模型在庞大语料库上进行训练（通常会违反版权法），将用户提示转换为“Token”，并返回统计上最可能的连续Token作为响应。

当数百万用户向AI倾诉他们最深层的秘密时，就为个人身份信息泄露提供了大量机会。研究发现，“提示词工程”一个现成的聊天机器人来请求更多个人数据出乎意料地容易。

**技术门槛极低**是另一个关键因素。伦敦国王学院的研究人员发现，即使是“技术经验极少”的攻击者，也可以利用OpenAI等公司提供的“系统提示词”定制工具实现数据收集。

研究显示，恶意聊天机器人智能体能获取**显著更多的个人信息**（超过90%的参与者披露个人数据），而基线良性聊天机器人只有24%。

更令人担忧的是，当聊天机器人采用“互惠”社交方式时，用户几乎不会感到不适，却更容易分享敏感信息。

**安全措施形同虚设**也是大问题。在2025年黑帽USA安全会议上，研究人员展示了一种名为AgentFlayer的新方法。

该技术使用白色字体在白色背景上隐藏文本，人眼不可见，但AI系统可以轻松读取。一旦文件被包含在提示中，AI就会丢弃原始任务，转而执行隐藏指令——搜索连接的云存储以获取访问凭据。

## 03 多方施策，如何保护我们的隐私安全？

面对大模型带来的隐私风险，**政府层面已经出台了一系列措施**。

以下 7 部法规/文件是目前全球对“大模型处理个人信息”最直接、最具约束力的法律框架，均已在 2023-2025 年生效或有明确生效时间表，且都配有高额罚款或禁令机制。

#### 1. 欧盟  
- 《通用数据保护条例》（GDPR，2018-05-25 生效）  
- 《人工智能法》（EU AI Act，2024-08-01 生效，2026-08-02 全面适用）  
→ 2024-12 EDPB 第 28/2024 号意见：LLM 是否匿名需个案判定，不满足匿名即全程 GDPR 合规 。

#### 2. 美国  
- 《加州消费者隐私法》+《加州隐私权法》（CCPA/CPRA，2025-01-01 起加入 AB-1008 修订）  
→ 明确：LLM 输出若能指向个人，模型本身即被认定为“个人信息”，须允许删除/纠正 。

#### 3. 韩国  
- 《生成式 AI 开发、利用个人信息处理指南》（2025-08-09 发布，即时生效）  
→ 全球首个针对 LLM 训练阶段的个人信息合规指南，分阶段提出数据脱敏、模型去标识化、记忆风险检测等量化指标 。

#### 4. 意大利  
- 意大利个人数据保护局（Garante）系列禁令与罚款（2023-03-31 起）  
→ 2023 年率先封禁某 LLM；2024-12 追加 1,500 万欧元罚款，要求上线年龄验证、一键删除个人数据 。

#### 5. 英国  
- 《英国数据保护法案》（UK GDPR，2018-05-25 生效，2024-07-17 修订）  
- 《在线安全法》（Online Safety Act，2023-10-26 生效）  
→ 对 18 岁以下用户数据用于训练大模型需“可验证家长同意”，违者最高全球营收 4% 或 1,800 万英镑 。

#### 6. 加拿大  
- 《个人信息保护与电子文件法》（PIPEDA，2001-01-01 生效，2024-06 纳入 AI 修订）  
→ 训练集含敏感个人信息须做“算法影响评估”（AIA），并向隐私专员备案；最高罚款 1,000 万加元或全球营业额 3% 。

#### 7. 巴西  
- 《通用数据保护法》（LGPD，2020-09-18 生效）  
→ 2024-07 巴西国家数据保护局（ANPD）首次对 LLM 训练数据开出 1.5 亿雷亚尔罚单，理由是“未进行数据保护影响评估（DPIA）且缺少合法基础” 。

从欧盟的 GDPR+AI Act 组合拳，到美、加、韩、意、英、巴各自的“本地化补丁”，全球已进入“大模型隐私合规竞赛”阶段；开发者若想在多国上线，必须同时满足“训练端合法来源 + 部署端可撤回/可解释/可删除”三大硬指标。

> 研究人员还建议需要开发保护机制，包括**警告用户数据收集的提示系统**，以及**部署能在聊天中检测个人信息的上下文感知算法**。

监管机构和平台提供商也应进行早期审计、提高透明度并制定更严格的规则防止秘密数据收集。

## 04 隐私保护需各方共同努力

大模型技术快速发展带来的隐私风险不容忽视，即使是**0.001%的虚假文本被采用**，其有害输出也会相应上升**7.2%**。

保护个人隐私不仅需要政府完善法律法规、企业加强技术防护，也需要每个人提高安全意识，在使用AI工具时保持必要的警惕和谨慎。

只有在各方共同努力下，我们才能在享受AI技术带来便利的同时，保护好我们的个人隐私不被侵犯。

**你在使用大模型的时候，被侵犯过个人隐私吗？欢迎在评论区分享你的经历。**


---

![](https://cos.python-office.com/ads/gzh/sub-py.jpg)